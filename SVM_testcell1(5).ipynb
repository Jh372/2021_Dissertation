{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "turned-extra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model\n",
      "Output data68.csv[1/40]\n",
      "Output data40.csv[2/40]\n",
      "Output data79.csv[3/40]\n",
      "Output data116.csv[4/40]\n",
      "Output data113.csv[5/40]\n",
      "Output data13.csv[6/40]\n",
      "Output data126.csv[7/40]\n",
      "Output data42.csv[8/40]\n",
      "Output data35.csv[9/40]\n",
      "Output data43.csv[10/40]\n",
      "Output data58.csv[11/40]\n",
      "Output data136.csv[12/40]\n",
      "Output data129.csv[13/40]\n",
      "Output data26.csv[14/40]\n",
      "Output data38.csv[15/40]\n",
      "Output data30.csv[16/40]\n",
      "Output data188.csv[17/40]\n",
      "Output data61.csv[18/40]\n",
      "Output data21.csv[19/40]\n",
      "Output data198.csv[20/40]\n",
      "Output data134.csv[21/40]\n",
      "Output data60.csv[22/40]\n",
      "Output data36.csv[23/40]\n",
      "Output data34.csv[24/40]\n",
      "Output data180.csv[25/40]\n",
      "Output data125.csv[26/40]\n",
      "Output data135.csv[27/40]\n",
      "Output data2.csv[28/40]\n",
      "Output data67.csv[29/40]\n",
      "Output data56.csv[30/40]\n",
      "Output data29.csv[31/40]\n",
      "Output data51.csv[32/40]\n",
      "Output data100.csv[33/40]\n",
      "Output data120.csv[34/40]\n",
      "Output data3.csv[35/40]\n",
      "Output data124.csv[36/40]\n",
      "Output data75.csv[37/40]\n",
      "Output data98.csv[38/40]\n",
      "Output data65.csv[39/40]\n",
      "Output data48.csv[40/40]\n",
      "Complete task\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "from glob import glob\n",
    "import os\n",
    "from os.path import join\n",
    "import random\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "Path = './step2/output5_2_csv/'\n",
    "# get file list\n",
    "# files = glob(join(img_src, '*.csv'))\n",
    "test_file = np.loadtxt('./test_file_list.csv', delimiter=',',skiprows=1, dtype='object')\n",
    "train_file = np.loadtxt('./train_file_list.csv', delimiter=',',skiprows=1, dtype='object' )\n",
    "# # split data into train and test sets(8:2)\n",
    "# train_file, test_file = train_test_split(files, train_size=0.8)\n",
    "# df1 = pd.DataFrame(train_file)\n",
    "# df2 = pd.DataFrame(test_file)\n",
    "# df1.to_csv('./train_file_list.csv', index=None)\n",
    "# df2.to_csv('./test_file_list.csv', index=None)\n",
    "# apply SVM\n",
    "for num in range(len(train_file)):\n",
    "    df = pd.read_csv(Path+train_file[num])\n",
    "    clf = svm.SVC()\n",
    "    train_X,train_y = df.iloc[:,:-1].values,df.iloc[:,-1].values\n",
    "    clf.fit(train_X,train_y)\n",
    "    print('Fitting '+str(num+1)+ '/' + str(len(train_file)))\n",
    "# output model\n",
    "print('Save model')\n",
    "joblib.dump(clf, './train5_2.learn')\n",
    "# load model\n",
    "print('Load model')\n",
    "clf4 = joblib.load('./train5_2.learn')\n",
    "# load test data\n",
    "savename = join('./','./evaluation5_5_2.xlsx')\n",
    "if os.path.exists(savename):\n",
    "    os.remove(savename)\n",
    "for num in range(len(test_file)):\n",
    "    df = pd.read_csv(Path + test_file[num])\n",
    "    test_X,test_y = df.iloc[:,:-1].values,df.iloc[:,-1].values\n",
    "    # result\n",
    "    predicted_y = clf4.predict(test_X)\n",
    "    print('Output ' +str(test_file[num]) +'['+ str(num+1) + '/' + str(len(test_file))+']')\n",
    "    # evaluation(accuracy)\n",
    "    report = classification_report(test_y, predicted_y, output_dict=True)\n",
    "    sheetname = os.path.splitext(os.path.basename(test_file[num]))[0]\n",
    "    result = pd.DataFrame(report).transpose()\n",
    "    if os.path.exists(savename):\n",
    "        with pd.ExcelWriter(savename, engine=\"openpyxl\", mode=\"a\") as writer:\n",
    "            result.to_excel(writer, sheet_name=sheetname)\n",
    "    else:\n",
    "        with pd.ExcelWriter(savename, engine=\"openpyxl\") as writer:\n",
    "            result.to_excel(writer, sheet_name=sheetname)\n",
    "print('Complete task')\n",
    "#     df_r = pd.DataFrame(report).transpose()\n",
    "#     df_r[num].to_csv = ('./evaluation_5_5.csv')\n",
    "#     print(score)\n",
    "#     score = metrics.accuracy_score(test_y, predicted_y)\n",
    "#     print(\"Score:\", score)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-delhi",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # import library\n",
    "# from glob import glob\n",
    "# import os\n",
    "# from os.path import join\n",
    "# import random\n",
    "# from sklearn import svm\n",
    "# from sklearn import datasets\n",
    "# import joblib\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn import metrics\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import openpyxl\n",
    "# import pprint\n",
    "# # load model\n",
    "# clf4 = joblib.load('./train.learn')\n",
    "# test_file = np.loadtxt('./test_file_list.csv', delimiter=',',skiprows=1, dtype='object')\n",
    "# # Predict test data\n",
    "# savename = join('./','./evaluation5_5.xlsx')\n",
    "# if os.path.exists(savename):\n",
    "#     os.remove(savename)\n",
    "# for num in range(len(test_file)):\n",
    "#     df = pd.read_csv(test_file[num])\n",
    "#     test_X,test_y = df.iloc[:,:-1].values,df.iloc[:,-1].values\n",
    "#     # result\n",
    "#     predicted_y = clf4.predict(test_X)\n",
    "#     # evaluation(accuracy)\n",
    "#     report = classification_report(test_y, predicted_y, output_dict=True)\n",
    "#     sheetname = os.path.splitext(os.path.basename(test_file[num]))[0]\n",
    "# #     sheetname = 'result'+ str(num)\n",
    "#     result = pd.DataFrame(report).transpose()\n",
    "#     if os.path.exists(savename):\n",
    "#         with pd.ExcelWriter(savename, engine=\"openpyxl\", mode=\"a\") as writer:\n",
    "#             result.to_excel(writer, sheet_name=sheetname)\n",
    "#     else:\n",
    "#         with pd.ExcelWriter(savename, engine=\"openpyxl\") as writer:\n",
    "#             result.to_excel(writer, sheet_name=sheetname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "geological-relaxation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 372.800025940 [sec]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/home/jh372/CS5014/python/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/cs/home/jh372/CS5014/python/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/cs/home/jh372/CS5014/python/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "def unit_test(filename):\n",
    "    tic()\n",
    "    df = pd.read_csv(filename) #elapsed time: 151.782285452 [sec]\n",
    "    X,y = df.iloc[:,:-1].values,df.iloc[:,-1].values\n",
    "    pre_y = clf4.predict(X)\n",
    "#     report = classification_report(y, pre_y, output_dict=True)\n",
    "#     result = pd.DataFrame(report).transpose()\n",
    "#     result.to_csv('./evaluation_5_5_test.csv')\n",
    "    toc()\n",
    "\n",
    "unit_test(test_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50df605",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.loadtxt('./test_file_list.csv', delimiter=',',skiprows=1, dtype='object')\n",
    "train = np.loadtxt('./train_file_list.csv', delimiter=',',skiprows=1, skipcolumns = 1, dtype='object' )\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "distinguished-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for counting time\n",
    "import time\n",
    "\n",
    "def tic():\n",
    "    #require  to import time\n",
    "    global start_time_tictoc\n",
    "    start_time_tictoc = time.time()\n",
    "\n",
    "\n",
    "def toc(tag=\"elapsed time\"):\n",
    "    if \"start_time_tictoc\" in globals():\n",
    "        print(\"{}: {:.9f} [sec]\".format(tag, time.time() - start_time_tictoc))\n",
    "    else:\n",
    "        print(\"tic has not been called\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-blink",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
